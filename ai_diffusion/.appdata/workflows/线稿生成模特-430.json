{
  "543": {
    "inputs": {
      "ckpt_name": "depth_anything_v2_vitl.pth",
      "resolution": 1024,
      "image": [
        "690",
        0
      ]
    },
    "class_type": "DepthAnythingV2Preprocessor",
    "_meta": {
      "title": "DepthAnythingV2æ·±åº¦é¢„å¤„ç†å™¨"
    }
  },
  "545": {
    "inputs": {
      "brightness": 0.6000000000000001,
      "contrast": 3,
      "saturation": 1.0000000000000002,
      "image": [
        "543",
        0
      ]
    },
    "class_type": "LayerColor: Brightness & Contrast",
    "_meta": {
      "title": "äº®åº¦/å¯¹æ¯”åº¦"
    }
  },
  "546": {
    "inputs": {
      "brightness": 0.6000000000000001,
      "contrast": 3,
      "saturation": 1.0000000000000002,
      "image": [
        "545",
        0
      ]
    },
    "class_type": "LayerColor: Brightness & Contrast",
    "_meta": {
      "title": "äº®åº¦/å¯¹æ¯”åº¦"
    }
  },
  "549": {
    "inputs": {
      "channel": "red",
      "image": [
        "546",
        0
      ]
    },
    "class_type": "ImageToMask",
    "_meta": {
      "title": "å›¾åƒåˆ°é®ç½©"
    }
  },
  "563": {
    "inputs": {
      "ckpt_name": "SDXL/juggernautXL_v9Rdphoto2Lightning.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "CheckpointåŠ è½½å™¨(ç®€æ˜“)"
    }
  },
  "564": {
    "inputs": {
      "seed": [
        "886",
        0
      ],
      "steps": 20,
      "cfg": 3.5,
      "sampler_name": "euler",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "730",
        0
      ],
      "positive": [
        "860",
        0
      ],
      "negative": [
        "860",
        1
      ],
      "latent_image": [
        "664",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "565": {
    "inputs": {
      "text": [
        "857",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "563",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç å™¨"
    }
  },
  "567": {
    "inputs": {
      "weight": 0.6000000000000001,
      "start_at": 0,
      "end_at": 1,
      "weight_type": "prompt is more important",
      "model": [
        "568",
        0
      ],
      "ipadapter": [
        "568",
        1
      ],
      "image": [
        "584",
        0
      ],
      "attn_mask": [
        "549",
        0
      ]
    },
    "class_type": "IPAdapter",
    "_meta": {
      "title": "åº”ç”¨IPAdapter"
    }
  },
  "568": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "563",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapteråŠ è½½å™¨"
    }
  },
  "570": {
    "inputs": {
      "samples": [
        "564",
        0
      ],
      "vae": [
        "563",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAEè§£ç "
    }
  },
  "575": {
    "inputs": {
      "strength": 0.6500000000000001,
      "start_percent": 0.07000000000000002,
      "end_percent": 1,
      "positive": [
        "565",
        0
      ],
      "negative": [
        "755",
        0
      ],
      "control_net": [
        "576",
        0
      ],
      "image": [
        "766",
        0
      ],
      "vae": [
        "563",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNetåº”ç”¨ï¼ˆæ—§ç‰ˆé«˜çº§ï¼‰"
    }
  },
  "576": {
    "inputs": {
      "control_net_name": "sdxl_cn/controlnet-union-promax-sdxl-1.0.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNetåŠ è½½å™¨"
    }
  },
  "580": {
    "inputs": {
      "clip_l": [
        "861",
        0
      ],
      "t5xxl": [
        "861",
        0
      ],
      "guidance": 4.5,
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "803",
        1
      ]
    },
    "class_type": "CLIPTextEncodeFlux",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç Flux"
    }
  },
  "581": {
    "inputs": {
      "width": 1280,
      "height": 1280,
      "upscale_method": "lanczos",
      "keep_proportion": true,
      "divisible_by": 2,
      "crop": "disabled",
      "image": [
        "690",
        0
      ]
    },
    "class_type": "ImageResizeKJ",
    "_meta": {
      "title": "å›¾åƒç¼©æ”¾ï¼ˆKJï¼‰"
    }
  },
  "582": {
    "inputs": {
      "ckpt_name": "flux/flux1-schnell-fp8.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "CheckpointåŠ è½½å™¨(ç®€æ˜“)"
    }
  },
  "583": {
    "inputs": {
      "conditioning": [
        "580",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "æ¡ä»¶é›¶åŒ–"
    }
  },
  "584": {
    "inputs": {
      "samples": [
        "585",
        0
      ],
      "vae": [
        "582",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAEè§£ç "
    }
  },
  "585": {
    "inputs": {
      "seed": [
        "886",
        0
      ],
      "steps": 6,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "beta",
      "denoise": 1,
      "model": [
        "803",
        0
      ],
      "positive": [
        "580",
        0
      ],
      "negative": [
        "583",
        0
      ],
      "latent_image": [
        "797",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "586": {
    "inputs": {
      "images": [
        "584",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "é¢„è§ˆå›¾åƒ"
    }
  },
  "664": {
    "inputs": {
      "width": [
        "690",
        1
      ],
      "height": [
        "690",
        2
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "ç©ºLatent"
    }
  },
  "666": {
    "inputs": {
      "mask": [
        "549",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "é®ç½©åˆ°å›¾åƒ"
    }
  },
  "667": {
    "inputs": {
      "images": [
        "666",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "é¢„è§ˆå›¾åƒ"
    }
  },
  "669": {
    "inputs": {
      "samples": [
        "673",
        0
      ],
      "vae": [
        "582",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAEè§£ç "
    }
  },
  "673": {
    "inputs": {
      "seed": 980001631158992,
      "steps": 6,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "beta",
      "denoise": 0.5500000000000002,
      "model": [
        "803",
        0
      ],
      "positive": [
        "580",
        0
      ],
      "negative": [
        "583",
        0
      ],
      "latent_image": [
        "675",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "675": {
    "inputs": {
      "pixels": [
        "570",
        0
      ],
      "vae": [
        "582",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAEç¼–ç "
    }
  },
  "685": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "upscale_method": "lanczos",
      "keep_proportion": true,
      "divisible_by": 2,
      "crop": "disabled",
      "image": [
        "669",
        0
      ]
    },
    "class_type": "ImageResizeKJ",
    "_meta": {
      "title": "å›¾åƒç¼©æ”¾ï¼ˆKJï¼‰"
    }
  },
  "690": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "upscale_method": "lanczos",
      "keep_proportion": true,
      "divisible_by": 2,
      "crop": "disabled",
      "image": [
        "897",
        0
      ]
    },
    "class_type": "ImageResizeKJ",
    "_meta": {
      "title": "å›¾åƒç¼©æ”¾ï¼ˆKJï¼‰"
    }
  },
  "693": {
    "inputs": {
      "negative": "Black stroke, (nsfw:1.5)ï¼Œ(more arm:1.2)ï¼Œ(more leg:1.2)ï¼Œ(look forward the camera:1.4),(glove:1.7),(cartoon:1.4), Leather material,hair, fur, false, dark, (overexposed: 1.2), distorted, low quality, (high contrast: 1.2), (wrinkled:1.2), (hanging: 1.3),(floating: 1.3), complex background, mottled background, (high reflection material: 1.1)ï¼Œ(high contrast:1.3), (dark skin:1.2)",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "easy negative",
    "_meta": {
      "title": "è´Ÿé¢æç¤ºè¯"
    }
  },
  "730": {
    "inputs": {
      "weight": 0.5000000000000001,
      "start_at": 0,
      "end_at": 1,
      "weight_type": "standard",
      "model": [
        "567",
        0
      ],
      "ipadapter": [
        "568",
        1
      ],
      "image": [
        "732",
        0
      ],
      "attn_mask": [
        "731",
        0
      ]
    },
    "class_type": "IPAdapter",
    "_meta": {
      "title": "åº”ç”¨IPAdapter"
    }
  },
  "731": {
    "inputs": {
      "mask": [
        "549",
        0
      ]
    },
    "class_type": "InvertMask (segment anything)",
    "_meta": {
      "title": "åè½¬é®ç½©"
    }
  },
  "732": {
    "inputs": {
      "size": "custom",
      "custom_width": 1408,
      "custom_height": 1408,
      "color": "#ffffff",
      "size_as": [
        "584",
        0
      ]
    },
    "class_type": "LayerUtility: ColorImage V2",
    "_meta": {
      "title": "çº¯è‰²å›¾åƒ_V2"
    }
  },
  "750": {
    "inputs": {
      "coarse": "disable",
      "resolution": 1408,
      "image": [
        "897",
        0
      ]
    },
    "class_type": "LineArtPreprocessor",
    "_meta": {
      "title": "LineArtè‰ºæœ¯çº¿é¢„å¤„ç†å™¨"
    }
  },
  "751": {
    "inputs": {
      "images": [
        "766",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "é¢„è§ˆå›¾åƒ"
    }
  },
  "752": {
    "inputs": {
      "prompt": [
        "867",
        0
      ],
      "system_instruction": [
        "855",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "model": [
        "753",
        0
      ],
      "tokenizer": [
        "753",
        1
      ]
    },
    "class_type": "Qwen2_Zho",
    "_meta": {
      "title": "â›±ï¸Qwen2"
    }
  },
  "753": {
    "inputs": {
      "model_name": "Qwen/Qwen2-7B-Instruct"
    },
    "class_type": "Qwen2_ModelLoader_Zho",
    "_meta": {
      "title": "â›±ï¸Qwen2 ModelLoader"
    }
  },
  "754": {
    "inputs": {
      "anything": [
        "752",
        0
      ]
    },
    "class_type": "easy showAnything",
    "_meta": {
      "title": "å±•ç¤ºä»»ä½•"
    }
  },
  "755": {
    "inputs": {
      "text": [
        "693",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "563",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç å™¨"
    }
  },
  "766": {
    "inputs": {
      "brightness": 3,
      "contrast": 1.0000000000000002,
      "saturation": 1.0000000000000002,
      "image": [
        "750",
        0
      ]
    },
    "class_type": "LayerColor: Brightness & Contrast",
    "_meta": {
      "title": "äº®åº¦/å¯¹æ¯”åº¦"
    }
  },
  "787": {
    "inputs": {
      "prompt": [
        "752",
        0
      ],
      "system_instruction": "æŠŠè¿™æ®µæè¿°è½¬ä¸ºé€‚é…sdxlå¤§æ¨¡å‹çš„promptï¼Œå…¨è‹±æ–‡ï¼Œåªè¾“å‡ºç»“æœï¼Œç¦æ­¢æœ‰æ ‡é¢˜å’Œç‰¹æ®Šç¬¦å·\n\nè¦æ±‚ï¼š\n1ï¼Œåªè¾“å‡ºç»“æœï¼Œè‹±æ–‡ï¼Œè‹±æ–‡ç¬¦å·ã€‚\n2ï¼Œç¦æ­¢æ ‡é¢˜å’Œç‰¹æ®Šç¬¦å·å’ŒåŒå¼•å·ã€‚\n3ï¼Œå¼ºè°ƒè¶…é€¼çœŸå›¾ç‰‡ï¼Œè¶…å†™å®é£ã€‚\n4ï¼Œ100å­—ç¬¦ä»¥å†…",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "model": [
        "753",
        0
      ],
      "tokenizer": [
        "753",
        1
      ]
    },
    "class_type": "Qwen2_Zho",
    "_meta": {
      "title": "â›±ï¸Qwen2"
    }
  },
  "788": {
    "inputs": {
      "text": [
        "857",
        0
      ]
    },
    "class_type": "ShowText|pysssss",
    "_meta": {
      "title": "å±•ç¤ºæ–‡æœ¬"
    }
  },
  "790": {
    "inputs": {
      "text": "å»æ‰æ’ç”»å’Œé»‘ç™½å›¾ç›¸å…³çš„æè¿°ï¼ŒæŠŠæè¿°æ”¹ä¸ºæ—¶å°šçš„æœé¥°ã€‚å¢åŠ é£æ ¼æè¿°ï¼šè¶…å†™å®é£æ ¼ï¼Œæ¸…æ™°çš„å¸ƒæ–™çº¹ç†ã€‚\n\nè¦æ±‚ï¼š\n1ï¼Œç®€æ´æ˜äº†ï¼Œåªè¾“å‡ºç­”æ¡ˆï¼Œçº¯è‹±æ–‡ã€‚\n2ï¼Œå»æ‰æ’ç”»ã€illustrationã€é»‘ç™½å›¾ç­‰æœ‰å…³å¡é€šæ’ç”»çš„ç”»é¢æè¿°ã€‚\n3ï¼Œå»æ‰æ‰€æœ‰é¢œè‰²æè¿°ï¼Œå¼ºåŒ–è¡£æœä¸Šçš„å›¾æ¡ˆæè¿°ã€‚\n4ï¼Œå¢åŠ æˆ–å¼ºåŒ–æè¿°ï¼š",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "CR Text",
    "_meta": {
      "title": "æ–‡æœ¬"
    }
  },
  "797": {
    "inputs": {
      "width": [
        "581",
        1
      ],
      "height": [
        "581",
        2
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "ç©ºLatent"
    }
  },
  "803": {
    "inputs": {
      "lora_name": "flux/portrait-photography.safetensors",
      "strength_model": 0.9500000000000002,
      "strength_clip": 1.0000000000000002,
      "model": [
        "582",
        0
      ],
      "clip": [
        "582",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "LoRAåŠ è½½å™¨"
    }
  },
  "828": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 829644618111795,
      "steps": 20,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 0.6500000000000001,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5000000000000001,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.9300000000000002,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7000000000000002,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": [
        "851",
        0
      ],
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "tiled_encode": {
        "__value__": [
          false,
          true
        ]
      },
      "tiled_decode": false,
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "image": [
        "898",
        0
      ],
      "model": [
        "862",
        0
      ],
      "clip": [
        "863",
        0
      ],
      "vae": [
        "865",
        0
      ],
      "positive": [
        "847",
        0
      ],
      "negative": [
        "848",
        0
      ],
      "bbox_detector": [
        "850",
        0
      ],
      "sam_model_opt": [
        "849",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "é¢éƒ¨ç»†åŒ–"
    }
  },
  "847": {
    "inputs": {
      "text": [
        "851",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "863",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç å™¨"
    }
  },
  "848": {
    "inputs": {
      "conditioning": [
        "847",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "æ¡ä»¶é›¶åŒ–"
    }
  },
  "849": {
    "inputs": {
      "model_name": "sam_vit_h_4b8939.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMåŠ è½½å™¨"
    }
  },
  "850": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "æ£€æµ‹åŠ è½½å™¨"
    }
  },
  "851": {
    "inputs": {
      "text": "Hyper-realistic facial close-up radiates luminous vitality, capturing an 18-year-old Eurasian beauty mid-laughter. Sun-kissed alabaster skin glows with inner warmth, capillaries beneath translucent cheeks pulsing like liquid coral under morning light. Her Slavic-high cheekbones catch golden halos where sunlight diffuses through downy peach fuzz, while the East Asian oval jawline reflects opalescent sheen reminiscent of moonstone. Almond eyes crinkle at the corners with uncontained mirth, irises transformed into molten topaz pools by direct sunlight - amber flecks igniting into solar flares against steel-gray depths.\n\nWind-tousled chestnut waves frame a face where European angularity and Asian softness merge: the sharp Cupid's bow of her lips arches upward into a smile so vivid it activates dimple creases, while fuller Asian lip contours glisten with moisture from recent laughter. Every epidermal detail conspires toward radiance - sweat droplets along her hairline refract prismatic light, vellus hairs on sunlit temples glowing like spun gold filaments. The Slavic nose bridge becomes a luminous ridge where sunlight fractures into spectral micro-beams across poreless skin, while faint epicanthic folds soften eye smiles into crescent moons.\n\nThis is genetic alchemy electrified by joy: sunlight penetrates to the dermal layer, revealing how golden undertones in her East Asian epidermis amplify Caucasian translucency into living bioluminescence. Even the shadows beneath lashes dance with refracted light particles, each microscopic skin valley catching highlights that transform facial topography into a 3D map of luminosity. A living portrait where multicultural beauty becomes pure energy, every skin texture vibrating with captured sunlight.",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "CR Text",
    "_meta": {
      "title": "æ–‡æœ¬"
    }
  },
  "852": {
    "inputs": {
      "text": [
        "875",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "CR Text",
    "_meta": {
      "title": "æ–‡æœ¬"
    }
  },
  "853": {
    "inputs": {
      "text1": [
        "790",
        0
      ],
      "text2": [
        "852",
        0
      ],
      "separator": ""
    },
    "class_type": "CR Text Concatenate",
    "_meta": {
      "title": "æ–‡æœ¬è”ç»“"
    }
  },
  "854": {
    "inputs": {
      "text": "\n5ï¼ŒæŠŠâ€œhyper-realistic photographâ€æ”¾åœ¨å¥é¦–ã€‚å¼ºè°ƒï¼šæ£šå†…å°„å½±ï¼Œå¹²å‡€çš„å…‰ç…§ï¼Œé€¼çœŸçš„ç…§ç‰‡ï¼Œè¶…å†™å®é£æ ¼ï¼Œè‡ªç„¶æŠ“æ‹ç…§ï¼Œæ¸…æ–°æ˜äº®ï¼Œä½å¯¹æ¯”åº¦ï¼ŒçœŸå®é«˜çº§çš„è´¨æ„Ÿï¼Œè¶…é«˜è´¨é‡ã€‚å·´é»æ—¶è£…å‘¨æ¨¡ç‰¹ï¼Œç™½çš™çš„çš®è‚¤ã€çœŸå®çš„çš®è‚¤è´¨æ„Ÿ,çœ¼ç›ä¸çœ‹å‘é•œå¤´,æ¼‚äº®çš„è„¸ã€å¼€æœ—çš„å¾®ç¬‘ã€å‡†ç¡®çš„äººä½“ç»“æ„ã€‚\n6ï¼Œç¦æ­¢åŒ…å«æ ‡é¢˜å’Œç‰¹æ®Šç¬¦å·ã€‚\n7ï¼Œæ ¼å¼ä¸ºé€‚é…fluxå¤§æ¨¡å‹çš„promptã€‚",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "CR Text",
    "_meta": {
      "title": "æ–‡æœ¬"
    }
  },
  "855": {
    "inputs": {
      "text1": [
        "853",
        0
      ],
      "text2": [
        "854",
        0
      ],
      "separator": ""
    },
    "class_type": "CR Text Concatenate",
    "_meta": {
      "title": "æ–‡æœ¬è”ç»“"
    }
  },
  "857": {
    "inputs": {
      "text1": [
        "858",
        0
      ],
      "text2": [
        "787",
        0
      ],
      "separator": ","
    },
    "class_type": "CR Text Concatenate",
    "_meta": {
      "title": "æ–‡æœ¬è”ç»“"
    }
  },
  "858": {
    "inputs": {
      "prompt": [
        "852",
        0
      ],
      "system_instruction": "æŠŠè¿™æ®µæè¿°è½¬ä¸ºé€‚é…sdxlå¤§æ¨¡å‹çš„promptï¼Œå…¨è‹±æ–‡ï¼Œåªè¾“å‡ºç»“æœï¼Œç¦æ­¢æœ‰æ ‡é¢˜å’Œç‰¹æ®Šç¬¦å·\n\nè¦æ±‚ï¼š\n1ï¼Œåªè¾“å‡ºç»“æœï¼Œè‹±æ–‡ï¼Œè‹±æ–‡ç¬¦å·ã€‚\n2ï¼Œç¦æ­¢æ ‡é¢˜å’Œç‰¹æ®Šç¬¦å·å’ŒåŒå¼•å·ã€‚",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "model": [
        "753",
        0
      ],
      "tokenizer": [
        "753",
        1
      ]
    },
    "class_type": "Qwen2_Zho",
    "_meta": {
      "title": "â›±ï¸Qwen2"
    }
  },
  "860": {
    "inputs": {
      "strength": 0.15000000000000002,
      "start_percent": 0.10000000000000002,
      "end_percent": 1,
      "positive": [
        "575",
        0
      ],
      "negative": [
        "575",
        1
      ],
      "control_net": [
        "576",
        0
      ],
      "image": [
        "666",
        0
      ],
      "vae": [
        "563",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNetåº”ç”¨ï¼ˆæ—§ç‰ˆé«˜çº§ï¼‰"
    }
  },
  "861": {
    "inputs": {
      "text1": [
        "858",
        0
      ],
      "text2": [
        "752",
        0
      ],
      "separator": ","
    },
    "class_type": "CR Text Concatenate",
    "_meta": {
      "title": "æ–‡æœ¬è”ç»“"
    }
  },
  "862": {
    "inputs": {
      "unet_name": "SuperMerged2.6.safetensors",
      "weight_dtype": "fp8_e4m3fn"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "UNETåŠ è½½å™¨"
    }
  },
  "863": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp8_e4m3fn.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "åŒCLIPåŠ è½½å™¨"
    }
  },
  "865": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "VAEåŠ è½½å™¨"
    }
  },
  "867": {
    "inputs": {
      "text": [
        "871",
        0
      ]
    },
    "class_type": "ShowText|pysssss",
    "_meta": {
      "title": "å±•ç¤ºæ–‡æœ¬"
    }
  },
  "868": {
    "inputs": {
      "prompt": "Please describe the color and pattern of this picture",
      "model": "qwen-vl-max",
      "system_message": "You are a helpful assistant.",
      "temperature": 0.7000000000000001,
      "max_tokens": 1024,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "image": [
        "874",
        0
      ]
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "LLMé«˜çº§èŠ‚ç‚¹"
    }
  },
  "871": {
    "inputs": {
      "prompt": "Please provide a detailed description of this image and output it in natural language",
      "model": "qwen-vl-max",
      "system_message": "You are a helpful assistant.",
      "temperature": 0.7000000000000001,
      "max_tokens": 1024,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "image": [
        "690",
        0
      ]
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "LLMé«˜çº§èŠ‚ç‚¹"
    }
  },
  "872": {
    "inputs": {},
    "class_type": "ETN_KritaCanvas",
    "_meta": {
      "title": "Krita Canvas"
    }
  },
  "873": {
    "inputs": {
      "images": [
        "879",
        0
      ]
    },
    "class_type": "ETN_KritaOutput",
    "_meta": {
      "title": "Krita Output"
    }
  },
  "874": {
    "inputs": {
      "name": "2. å‚è€ƒå›¾ï¼ˆreference imageï¼‰(tips: è¯·é€‰æ‹©æ‰€éœ€è¦å‚è€ƒçš„å›¾ç‰‡)"
    },
    "class_type": "ETN_KritaImageLayer",
    "_meta": {
      "title": "Krita Image Layer"
    }
  },
  "875": {
    "inputs": {
      "boolean": [
        "878",
        0
      ],
      "on_true": [
        "894",
        0
      ],
      "on_false": [
        "876",
        0
      ]
    },
    "class_type": "Switch any [Crystools]",
    "_meta": {
      "title": "åˆ‡æ¢ä»»æ„"
    }
  },
  "876": {
    "inputs": {
      "String": [
        "877",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "å­—ç¬¦ä¸²"
    }
  },
  "877": {
    "inputs": {
      "name": "3. æç¤ºè¯ï¼ˆpromptï¼‰(tips:è¯·è¾“å…¥ä½ å¯¹çº¿ç¨¿çš„æè¿°ã€‚)",
      "type": "text",
      "default": "é»„è‰²çš„ä¸Šè¡£ï¼Œçº¢è‰²çŸ­å‘ï¼Œé¢å¸¦å¾®ç¬‘ï¼Œå¹´è½»ã€å¥åº·ã€æ´»æ³¼çš„18å²ä¸­ä¿„æ··è¡€ç¾å¥³ã€‚",
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "878": {
    "inputs": {
      "name": "1. å¼€å¯å‚è€ƒå›¾å‚è€ƒï¼ˆOpen reference image referenceï¼‰(tips: å¼€å¯åï¼Œè¾“å…¥çš„æç¤ºè¯å°†ä¸èµ·ä½œç”¨ï¼ŒAIå°†é€šè¿‡ä¸Šä¼ çš„å‚è€ƒå›¾æ§åˆ¶ç”Ÿæˆ)",
      "type": "toggle",
      "default": true,
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "879": {
    "inputs": {
      "width": [
        "872",
        1
      ],
      "height": [
        "872",
        2
      ],
      "crop": "disabled",
      "upscale_method": "nearest-exact",
      "lock_aspect_ratio": true,
      "image": [
        "828",
        0
      ]
    },
    "class_type": "EG_TX_SFBLS",
    "_meta": {
      "title": "2ğŸ•Image scaling lock"
    }
  },
  "886": {
    "inputs": {
      "seed": 135121212950668
    },
    "class_type": "easy seed",
    "_meta": {
      "title": "éšæœºç§"
    }
  },
  "891": {
    "inputs": {
      "action": "append",
      "tidy_tags": "yes",
      "text_a": [
        "895",
        0
      ],
      "text_b": [
        "896",
        0
      ],
      "text_c": "",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "å­—ç¬¦ä¸²æ“ä½œ"
    }
  },
  "892": {
    "inputs": {
      "prompt": [
        "891",
        0
      ],
      "model": "qwen-max",
      "system_message": "1. Please adjust the prompt words according to the description given to you. The first paragraph provides an overall description of the clothing, movements, background, shoes, etc. worn by the model, while the second paragraph describes the color scheme of the clothes worn by the model.\n3. Extract the color pattern from the second paragraph description and replace the clothing color scheme from the first paragraph. If there is a handbag, the handbag color scheme should also be applied.\n4. The use of black and white lines, sketches, drawings, and other descriptions is prohibited, and only models depicting real people are allowed. Please output prompt words in the following output example format.\nOutput example: A young, healthy, and lively 18-year-old Chinese Russian mixed race beauty wearing a yellow striped shirt, with short red hair and a smile on her face.",
      "temperature": 0.7000000000000001,
      "max_tokens": 1024,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "LLMé«˜çº§èŠ‚ç‚¹"
    }
  },
  "894": {
    "inputs": {
      "text": [
        "892",
        0
      ]
    },
    "class_type": "ShowText|pysssss",
    "_meta": {
      "title": "å±•ç¤ºæ–‡æœ¬"
    }
  },
  "895": {
    "inputs": {
      "action": "append",
      "tidy_tags": "no",
      "text_a": "ç¬¬ä¸€æ®µï¼š",
      "text_b": [
        "871",
        0
      ],
      "text_c": "",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "å­—ç¬¦ä¸²æ“ä½œ"
    }
  },
  "896": {
    "inputs": {
      "action": "append",
      "tidy_tags": "no",
      "text_a": "ç¬¬äºŒæ®µï¼š",
      "text_b": [
        "868",
        0
      ],
      "text_c": "",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "å­—ç¬¦ä¸²æ“ä½œ"
    }
  },
  "897": {
    "inputs": {
      "anything": [
        "872",
        0
      ]
    },
    "class_type": "easy cleanGpuUsed",
    "_meta": {
      "title": "æ¸…ç†GPUå ç”¨"
    }
  },
  "898": {
    "inputs": {
      "anything": [
        "685",
        0
      ]
    },
    "class_type": "easy cleanGpuUsed",
    "_meta": {
      "title": "æ¸…ç†GPUå ç”¨"
    }
  }
}