{
  "1": {
    "inputs": {
      "ckpt_name": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "CheckpointåŠ è½½å™¨(ç®€æ˜“)"
    }
  },
  "2": {
    "inputs": {
      "seed": [
        "181",
        0
      ],
      "steps": 20,
      "cfg": 2.5,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "43",
        0
      ],
      "positive": [
        "67",
        0
      ],
      "negative": [
        "67",
        1
      ],
      "latent_image": [
        "17",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "3": {
    "inputs": {
      "text": [
        "170",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç å™¨"
    }
  },
  "7": {
    "inputs": {
      "text": [
        "173",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç å™¨"
    }
  },
  "15": {
    "inputs": {
      "samples": [
        "2",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAEè§£ç "
    }
  },
  "17": {
    "inputs": {
      "width": [
        "21",
        0
      ],
      "height": [
        "21",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "ç©ºLatent"
    }
  },
  "21": {
    "inputs": {
      "image": [
        "27",
        0
      ]
    },
    "class_type": "GetImageSize+",
    "_meta": {
      "title": "è·å–å›¾åƒå°ºå¯¸"
    }
  },
  "27": {
    "inputs": {
      "max_width": 1280,
      "max_height": 1280,
      "min_width": 0,
      "min_height": 0,
      "crop_if_required": "no",
      "images": [
        "112",
        0
      ]
    },
    "class_type": "ConstrainImage|pysssss",
    "_meta": {
      "title": "é™åˆ¶å›¾åƒåŒºåŸŸ"
    }
  },
  "43": {
    "inputs": {
      "weight": [
        "195",
        0
      ],
      "weight_type": "strong style transfer",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 0.5000000000000001,
      "embeds_scaling": "V only",
      "model": [
        "44",
        0
      ],
      "ipadapter": [
        "44",
        1
      ],
      "image": [
        "190",
        0
      ],
      "image_negative": [
        "112",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "åº”ç”¨IPAdapterï¼ˆé«˜çº§ï¼‰"
    }
  },
  "44": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "110",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapteråŠ è½½å™¨"
    }
  },
  "67": {
    "inputs": {
      "strength": 0.9000000000000001,
      "start_percent": 0,
      "end_percent": 0.9000000000000002,
      "positive": [
        "3",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "control_net": [
        "68",
        0
      ],
      "image": [
        "73",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNetåº”ç”¨ï¼ˆæ—§ç‰ˆé«˜çº§ï¼‰"
    }
  },
  "68": {
    "inputs": {
      "control_net_name": "sdxl/diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNetåŠ è½½å™¨"
    }
  },
  "73": {
    "inputs": {
      "preprocessor": "TEEDPreprocessor",
      "resolution": 512,
      "image": [
        "27",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "Auxé›†æˆé¢„å¤„ç†å™¨"
    }
  },
  "110": {
    "inputs": {
      "lora_name": "ç»‡ç‰©ç½‘é¢é‹ XL-000016.safetensors",
      "strength_model": 0.8000000000000002,
      "model": [
        "1",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoRAåŠ è½½å™¨(ä»…æ¨¡å‹)"
    }
  },
  "112": {
    "inputs": {
      "aspect_ratio": "original",
      "proportional_width": 1,
      "proportional_height": 1,
      "fit": "letterbox",
      "method": "lanczos",
      "round_to_multiple": "8",
      "scale_to_side": "width",
      "scale_to_length": 1024,
      "background_color": "#ffffff",
      "image": [
        "189",
        0
      ]
    },
    "class_type": "LayerUtility: ImageScaleByAspectRatio V2",
    "_meta": {
      "title": "æŒ‰å®½é«˜æ¯”ç¼©æ”¾_V2"
    }
  },
  "138": {
    "inputs": {
      "style_model_name": "flux1-redux-dev.safetensors"
    },
    "class_type": "StyleModelLoader",
    "_meta": {
      "title": "é£æ ¼æ¨¡å‹åŠ è½½å™¨"
    }
  },
  "139": {
    "inputs": {
      "clip_name": "sigclip_vision_patch14_384.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "CLIPè§†è§‰åŠ è½½å™¨"
    }
  },
  "140": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp16.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "åŒCLIPåŠ è½½å™¨"
    }
  },
  "141": {
    "inputs": {
      "lora_stack": [
        "174",
        0
      ],
      "model": [
        "152",
        0
      ],
      "optional_clip": [
        "140",
        0
      ]
    },
    "class_type": "easy loraStackApply",
    "_meta": {
      "title": "åº”ç”¨ LoRA å †"
    }
  },
  "142": {
    "inputs": {
      "model_name": "OmniSR_X2_DIV2K.safetensors"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "æ”¾å¤§æ¨¡å‹åŠ è½½å™¨"
    }
  },
  "143": {
    "inputs": {
      "text": [
        "170",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "140",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIPæ–‡æœ¬ç¼–ç å™¨"
    }
  },
  "144": {
    "inputs": {
      "downsampling_factor": 1,
      "downsampling_function": "bicubic",
      "mode": "keep aspect ratio",
      "weight": 1,
      "autocrop_margin": 0.10000000000000002,
      "conditioning": [
        "143",
        0
      ],
      "style_model": [
        "138",
        0
      ],
      "clip_vision": [
        "139",
        0
      ],
      "image": [
        "15",
        0
      ]
    },
    "class_type": "ReduxAdvanced",
    "_meta": {
      "title": "ReduxAdvanced"
    }
  },
  "147": {
    "inputs": {
      "samples": [
        "158",
        0
      ],
      "vae": [
        "157",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAEè§£ç "
    }
  },
  "148": {
    "inputs": {
      "images": [
        "147",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "é¢„è§ˆå›¾åƒ"
    }
  },
  "152": {
    "inputs": {
      "unet_name": "SuperMerged2.6.safetensors",
      "weight_dtype": "fp8_e4m3fn"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "UNETåŠ è½½å™¨"
    }
  },
  "153": {
    "inputs": {
      "conditioning": [
        "144",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "æ¡ä»¶é›¶åŒ–"
    }
  },
  "154": {
    "inputs": {
      "guidance": 3.5,
      "conditioning": [
        "144",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "Fluxå¼•å¯¼"
    }
  },
  "157": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "VAEåŠ è½½å™¨"
    }
  },
  "158": {
    "inputs": {
      "seed": 1040180462665087,
      "steps": 25,
      "cfg": 1,
      "sampler_name": "uni_pc_bh2",
      "scheduler": "sgm_uniform",
      "denoise": 0.7000000000000002,
      "model": [
        "141",
        0
      ],
      "positive": [
        "176",
        0
      ],
      "negative": [
        "176",
        1
      ],
      "latent_image": [
        "161",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "Ké‡‡æ ·å™¨"
    }
  },
  "161": {
    "inputs": {
      "pixels": [
        "167",
        0
      ],
      "vae": [
        "157",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAEç¼–ç "
    }
  },
  "166": {
    "inputs": {
      "upscale_method": "lanczos",
      "scale_by": 0.7500000000000001,
      "image": [
        "15",
        0
      ]
    },
    "class_type": "ImageScaleBy",
    "_meta": {
      "title": "å›¾åƒæŒ‰ç³»æ•°ç¼©æ”¾"
    }
  },
  "167": {
    "inputs": {
      "upscale_model": [
        "142",
        0
      ],
      "image": [
        "166",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "å›¾åƒé€šè¿‡æ¨¡å‹æ”¾å¤§"
    }
  },
  "170": {
    "inputs": {
      "action": "append",
      "tidy_tags": "yes",
      "text_a": "",
      "text_b": "(Pure background:1.5)",
      "text_c": [
        "191",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "å­—ç¬¦ä¸²æ“ä½œ"
    }
  },
  "173": {
    "inputs": {
      "String": "reflect lightï¼Œtext, watermark,ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2),Avoid images that are blurry,poorly lit,or lack details. No other objects or distractions in the frame. Avoid low-quality textures,dull colors,and flat lighting,, (EasyNegative:1.2),badhandv4,NSFW, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, (ugly:1.331), (duplicate:1.331), (morbid:1.21), (mutilated:1.21), (tranny:1.331), mutated hands, (poorly drawn hands:1.5), blurry, (bad anatomy:1.21), (bad proportions:1.331), extra limbs, (disfigured:1.331), (missing arms:1.331), (extra legs:1.331), (fused fingers:1.61051), (too many fingers:1.61051), (unclear eyes:1.331), lowers, bad hands, missing fingers, extra digit,bad hands, missing fingers, (((extra arms and legs))),lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "å­—ç¬¦ä¸²"
    }
  },
  "174": {
    "inputs": {
      "toggle": false,
      "mode": "simple",
      "num_loras": 2,
      "lora_1_name": "flux_loras/external_paoxie_sole.safetensors",
      "lora_1_strength": 0.30000000000000004,
      "lora_1_model_strength": 1.0000000000000002,
      "lora_1_clip_strength": 1.0000000000000002,
      "lora_2_name": "flux/ç»‡ç‰©çº¹ç†flux v1000028.safetensors",
      "lora_2_strength": 0.5000000000000001,
      "lora_2_model_strength": 1.0000000000000002,
      "lora_2_clip_strength": 1.0000000000000002,
      "lora_3_name": "None",
      "lora_3_strength": 1.0000000000000002,
      "lora_3_model_strength": 1.0000000000000002,
      "lora_3_clip_strength": 1.0000000000000002,
      "lora_4_name": "None",
      "lora_4_strength": 1.0000000000000002,
      "lora_4_model_strength": 1.0000000000000002,
      "lora_4_clip_strength": 1.0000000000000002,
      "lora_5_name": "None",
      "lora_5_strength": 1.0000000000000002,
      "lora_5_model_strength": 1.0000000000000002,
      "lora_5_clip_strength": 1.0000000000000002,
      "lora_6_name": "None",
      "lora_6_strength": 1.0000000000000002,
      "lora_6_model_strength": 1.0000000000000002,
      "lora_6_clip_strength": 1.0000000000000002,
      "lora_7_name": "None",
      "lora_7_strength": 1.0000000000000002,
      "lora_7_model_strength": 1.0000000000000002,
      "lora_7_clip_strength": 1.0000000000000002,
      "lora_8_name": "None",
      "lora_8_strength": 1.0000000000000002,
      "lora_8_model_strength": 1.0000000000000002,
      "lora_8_clip_strength": 1.0000000000000002,
      "lora_9_name": "None",
      "lora_9_strength": 1.0000000000000002,
      "lora_9_model_strength": 1.0000000000000002,
      "lora_9_clip_strength": 1.0000000000000002,
      "lora_10_name": "None",
      "lora_10_strength": 1.0000000000000002,
      "lora_10_model_strength": 1.0000000000000002,
      "lora_10_clip_strength": 1.0000000000000002
    },
    "class_type": "easy loraStack",
    "_meta": {
      "title": "ç®€æ˜“Loraå †"
    }
  },
  "176": {
    "inputs": {
      "strength": 0.45000000000000007,
      "start_percent": 0,
      "end_percent": 0.4500000000000001,
      "positive": [
        "154",
        0
      ],
      "negative": [
        "153",
        0
      ],
      "control_net": [
        "177",
        0
      ],
      "image": [
        "178",
        0
      ],
      "vae": [
        "157",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNetåº”ç”¨ï¼ˆæ—§ç‰ˆé«˜çº§ï¼‰"
    }
  },
  "177": {
    "inputs": {
      "control_net_name": "cannys/controlnet_canny_external_paoxie_sole/diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNetåŠ è½½å™¨"
    }
  },
  "178": {
    "inputs": {
      "preprocessor": "PyraCannyPreprocessor",
      "resolution": 512,
      "image": [
        "167",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "Auxé›†æˆé¢„å¤„ç†å™¨"
    }
  },
  "181": {
    "inputs": {
      "seed": 728940021698944
    },
    "class_type": "easy seed",
    "_meta": {
      "title": "éšæœºç§"
    }
  },
  "189": {
    "inputs": {},
    "class_type": "ETN_KritaCanvas",
    "_meta": {
      "title": "Krita Canvas"
    }
  },
  "190": {
    "inputs": {
      "name": "1. å‚è€ƒå›¾ï¼ˆReference imageï¼‰(tips: è¯·é€‰æ‹©æ‰€éœ€è¦å‚è€ƒçš„å›¾ç‰‡)"
    },
    "class_type": "ETN_KritaImageLayer",
    "_meta": {
      "title": "Krita Image Layer"
    }
  },
  "191": {
    "inputs": {
      "boolean": [
        "194",
        0
      ],
      "on_true": [
        "204",
        0
      ],
      "on_false": [
        "206",
        0
      ]
    },
    "class_type": "Switch any [Crystools]",
    "_meta": {
      "title": "åˆ‡æ¢ä»»æ„"
    }
  },
  "192": {
    "inputs": {
      "String": [
        "193",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "å­—ç¬¦ä¸²"
    }
  },
  "193": {
    "inputs": {
      "name": "3. æç¤ºè¯ï¼ˆpromptï¼‰(tips: è¯·è¾“å…¥ä½ å¯¹çº¿ç¨¿çš„æè¿°ã€‚)",
      "type": "prompt (positive)",
      "default": "",
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "194": {
    "inputs": {
      "name": "2. å¼€å¯å‚è€ƒå›¾å‚è€ƒï¼ˆOpen reference image referenceï¼‰(tips:å¼€å¯åï¼Œè¾“å…¥çš„æç¤ºè¯å°†ä¸èµ·ä½œç”¨ï¼ŒAIå°†é€šè¿‡ä¸Šä¼ çš„å‚è€ƒå›¾æ§åˆ¶ç”Ÿæˆ)",
      "type": "toggle",
      "default": true,
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "195": {
    "inputs": {
      "boolean": [
        "194",
        0
      ],
      "on_true": [
        "196",
        0
      ],
      "on_false": [
        "197",
        0
      ]
    },
    "class_type": "Switch any [Crystools]",
    "_meta": {
      "title": "åˆ‡æ¢ä»»æ„"
    }
  },
  "196": {
    "inputs": {
      "Number": "0.5"
    },
    "class_type": "Float",
    "_meta": {
      "title": "æµ®ç‚¹"
    }
  },
  "197": {
    "inputs": {
      "Number": "0"
    },
    "class_type": "Float",
    "_meta": {
      "title": "æµ®ç‚¹"
    }
  },
  "198": {
    "inputs": {
      "images": [
        "199",
        0
      ]
    },
    "class_type": "ETN_KritaOutput",
    "_meta": {
      "title": "Krita Output"
    }
  },
  "199": {
    "inputs": {
      "width": [
        "189",
        1
      ],
      "height": [
        "189",
        2
      ],
      "crop": "disabled",
      "upscale_method": "nearest-exact",
      "lock_aspect_ratio": true,
      "image": [
        "147",
        0
      ]
    },
    "class_type": "EG_TX_SFBLS",
    "_meta": {
      "title": "2ğŸ•Image scaling lock"
    }
  },
  "204": {
    "inputs": {
      "model": "qwen2.5-vl-72b-instruct",
      "system_prompt": "You are an AI assistant capable of analyzing images. Please carefully observe the image and provide a detailed and accurate description based on the user's question.",
      "user_prompt": "Extract key design elements for footwear design from the image, including but not limited to: colors, design style, patterns, textures, and design concepts.\nï»¿\nTransform the extracted design elements into shoe designs with detailed descriptions, generating optimized prompt text suitable for the SDXL model.\nï»¿\nRequirements:\nï»¿\nThe shoes must exclusively use woven mesh material/texture.\nï»¿\nThe prompt text must be concise, precise and compliant with SDXL model input specifications.\nï»¿\nNo need to explain or describe anything else, no output of your thought process, no output of character description, only describe the appearance of the final designed shoe,The material of the shoe body must be described and given priorityï¼Œ and only appear text that matches the appearance of the shoe.\nï»¿\nOutput example:\nThe shoe features a sleek, modern design with a predominantly white upper constructed from woven mesh material for breathability and flexibility. The midsole is accentuated with vibrant pink stripes, adding a pop of color to the otherwise monochrome palette. The outsole is designed with a dynamic, sculpted pattern in shades of gray and pink, providing both style and traction. The overall aesthetic is sporty and stylish, The background should be pure white,suitable for both athletic activities and casual wear.",
      "max_tokens": 1024,
      "temperature": 0.7000000000000002,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "images": [
        "190",
        0
      ]
    },
    "class_type": "BizyAirSiliconCloudVLMAPI",
    "_meta": {
      "title": "â˜ï¸BizyAir SiliconCloud VLM API"
    }
  },
  "206": {
    "inputs": {
      "text_1": [
        "207",
        0
      ],
      "text_2": [
        "208",
        0
      ]
    },
    "class_type": "FusionText",
    "_meta": {
      "title": "èåˆæ–‡æœ¬"
    }
  },
  "207": {
    "inputs": {
      "String": "HD photography, HD product rendering, 4K, high qualityï¼Œ no background,advanced upper design, high-quality product design, exquisite shoelaces, (white background: 1.15)",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "å­—ç¬¦ä¸²"
    }
  },
  "208": {
    "inputs": {
      "prompt": [
        "192",
        0
      ],
      "model": "qwen-plus",
      "system_message": "You are creating a prompt for Stable Diffusion to generate an image. First step: understand the input and generate a text prompt for the input. Second step: only respond in English with the prompt itself in phrase, but embellish it as needed but keep it under 50 tokens.",
      "temperature": 0.7000000000000001,
      "max_tokens": 1024,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "LLMé«˜çº§èŠ‚ç‚¹"
    }
  }
}