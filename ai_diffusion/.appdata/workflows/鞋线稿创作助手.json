{
  "0": {
    "inputs": {
      "ckpt_name": "SDXL/juggernautXL_v9Rdphoto2Lightning.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Checkpoint\u52a0\u8f7d\u5668(\u7b80\u6613)"
    }
  },
  "1": {
    "inputs": {
      "control_net_name": "sdxl/diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNet\u52a0\u8f7d\u5668"
    }
  },
  "2": {
    "inputs": {
      "lora_name": "\u7ec7\u7269\u7f51\u9762\u978b XL-000016.safetensors",
      "strength_model": 1.0000000000000002,
      "model": [
        "0",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoRA\u52a0\u8f7d\u5668(\u4ec5\u6a21\u578b)"
    }
  },
  "3": {
    "inputs": {
      "style_model_name": "flux1-redux-dev.safetensors"
    },
    "class_type": "StyleModelLoader",
    "_meta": {
      "title": "\u98ce\u683c\u6a21\u578b\u52a0\u8f7d\u5668"
    }
  },
  "4": {
    "inputs": {
      "clip_name": "sigclip_vision_patch14_384.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "CLIP\u89c6\u89c9\u52a0\u8f7d\u5668"
    }
  },
  "5": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp16.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "\u53ccCLIP\u52a0\u8f7d\u5668"
    }
  },
  "6": {
    "inputs": {
      "model_name": "OmniSR_X2_DIV2K.safetensors"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "\u653e\u5927\u6a21\u578b\u52a0\u8f7d\u5668"
    }
  },
  "7": {
    "inputs": {
      "unet_name": "SuperMerged2.6.safetensors",
      "weight_dtype": "fp8_e4m3fn"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "UNET\u52a0\u8f7d\u5668"
    }
  },
  "8": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "VAE\u52a0\u8f7d\u5668"
    }
  },
  "9": {
    "inputs": {
      "String": "reflect light\uff0ctext, watermark,ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2),Avoid images that are blurry,poorly lit,or lack details. No other objects or distractions in the frame. Avoid low-quality textures,dull colors,and flat lighting,, (EasyNegative:1.2),badhandv4,NSFW, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, (ugly:1.331), (duplicate:1.331), (morbid:1.21), (mutilated:1.21), (tranny:1.331), mutated hands, (poorly drawn hands:1.5), blurry, (bad anatomy:1.21), (bad proportions:1.331), extra limbs, (disfigured:1.331), (missing arms:1.331), (extra legs:1.331), (fused fingers:1.61051), (too many fingers:1.61051), (unclear eyes:1.331), lowers, bad hands, missing fingers, extra digit,bad hands, missing fingers, (((extra arms and legs))),lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32"
    }
  },
  "10": {
    "inputs": {
      "toggle": false,
      "mode": "simple",
      "num_loras": 2,
      "lora_1_name": "flux_loras/external_paoxie_sole.safetensors",
      "lora_1_strength": 0.30000000000000004,
      "lora_1_model_strength": 1.0000000000000002,
      "lora_1_clip_strength": 1.0000000000000002,
      "lora_2_name": "flux/\u7ec7\u7269\u7eb9\u7406flux v1000028.safetensors",
      "lora_2_strength": 0.5000000000000001,
      "lora_2_model_strength": 1.0000000000000002,
      "lora_2_clip_strength": 1.0000000000000002,
      "lora_3_name": "None",
      "lora_3_strength": 1.0000000000000002,
      "lora_3_model_strength": 1.0000000000000002,
      "lora_3_clip_strength": 1.0000000000000002,
      "lora_4_name": "None",
      "lora_4_strength": 1.0000000000000002,
      "lora_4_model_strength": 1.0000000000000002,
      "lora_4_clip_strength": 1.0000000000000002,
      "lora_5_name": "None",
      "lora_5_strength": 1.0000000000000002,
      "lora_5_model_strength": 1.0000000000000002,
      "lora_5_clip_strength": 1.0000000000000002,
      "lora_6_name": "None",
      "lora_6_strength": 1.0000000000000002,
      "lora_6_model_strength": 1.0000000000000002,
      "lora_6_clip_strength": 1.0000000000000002,
      "lora_7_name": "None",
      "lora_7_strength": 1.0000000000000002,
      "lora_7_model_strength": 1.0000000000000002,
      "lora_7_clip_strength": 1.0000000000000002,
      "lora_8_name": "None",
      "lora_8_strength": 1.0000000000000002,
      "lora_8_model_strength": 1.0000000000000002,
      "lora_8_clip_strength": 1.0000000000000002,
      "lora_9_name": "None",
      "lora_9_strength": 1.0000000000000002,
      "lora_9_model_strength": 1.0000000000000002,
      "lora_9_clip_strength": 1.0000000000000002,
      "lora_10_name": "None",
      "lora_10_strength": 1.0000000000000002,
      "lora_10_model_strength": 1.0000000000000002,
      "lora_10_clip_strength": 1.0000000000000002
    },
    "class_type": "easy loraStack",
    "_meta": {
      "title": "\u7b80\u6613Lora\u5806"
    }
  },
  "11": {
    "inputs": {
      "control_net_name": "cannys/controlnet_canny_external_paoxie_sole/diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNet\u52a0\u8f7d\u5668"
    }
  },
  "12": {
    "inputs": {
      "seed": 728940021698944
    },
    "class_type": "easy seed",
    "_meta": {
      "title": "\u968f\u673a\u79cd"
    }
  },
  "13": {
    "inputs": {},
    "class_type": "ETN_KritaCanvas",
    "_meta": {
      "title": "Krita Canvas"
    }
  },
  "14": {
    "inputs": {
      "name": "1. \u53c2\u8003\u56fe\uff08Reference image\uff09(tips: \u8bf7\u9009\u62e9\u6240\u9700\u8981\u53c2\u8003\u7684\u56fe\u7247)"
    },
    "class_type": "ETN_KritaImageLayer",
    "_meta": {
      "title": "Krita Image Layer"
    }
  },
  "15": {
    "inputs": {
      "name": "3. \u63d0\u793a\u8bcd\uff08prompt\uff09(tips: \u8bf7\u8f93\u5165\u4f60\u5bf9\u7ebf\u7a3f\u7684\u63cf\u8ff0\u3002)",
      "type": "prompt (positive)",
      "default": "",
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "16": {
    "inputs": {
      "name": "2. \u5f00\u542f\u53c2\u8003\u56fe\u53c2\u8003\uff08Open reference image reference\uff09(tips:\u5f00\u542f\u540e\uff0c\u8f93\u5165\u7684\u63d0\u793a\u8bcd\u5c06\u4e0d\u8d77\u4f5c\u7528\uff0cAI\u5c06\u901a\u8fc7\u4e0a\u4f20\u7684\u53c2\u8003\u56fe\u63a7\u5236\u751f\u6210)(on:3-off)(off:3-on)",
      "type": "toggle",
      "default": true,
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "17": {
    "inputs": {
      "Number": "0.5"
    },
    "class_type": "Float",
    "_meta": {
      "title": "\u6d6e\u70b9"
    }
  },
  "18": {
    "inputs": {
      "Number": "0"
    },
    "class_type": "Float",
    "_meta": {
      "title": "\u6d6e\u70b9"
    }
  },
  "19": {
    "inputs": {
      "model": "qwen2.5-vl-72b-instruct",
      "system_prompt": "You are an AI assistant capable of analyzing images. Please carefully observe the image and provide a detailed and accurate description based on the user's question.",
      "user_prompt": "Extract key design elements for footwear design from the image, including but not limited to: colors, design style, patterns, textures, and design concepts.\n\ufeff\nTransform the extracted design elements into shoe designs with detailed descriptions, generating optimized prompt text suitable for the SDXL model.\n\ufeff\nRequirements:\n\ufeff\nThe shoes must exclusively use woven mesh material/texture.\n\ufeff\nThe prompt text must be concise, precise and compliant with SDXL model input specifications.\n\ufeff\nNo need to explain or describe anything else, no output of your thought process, no output of character description, only describe the appearance of the final designed shoe,The material of the shoe body must be described and given priority\uff0c and only appear text that matches the appearance of the shoe.\n\ufeff\nOutput example:\nThe shoe features a sleek, modern design with a predominantly white upper constructed from woven mesh material for breathability and flexibility. The midsole is accentuated with vibrant pink stripes, adding a pop of color to the otherwise monochrome palette. The outsole is designed with a dynamic, sculpted pattern in shades of gray and pink, providing both style and traction. The overall aesthetic is sporty and stylish, The background should be pure white,suitable for both athletic activities and casual wear.",
      "max_tokens": 1024,
      "temperature": 0.7000000000000002,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "images": [
        "14",
        0
      ]
    },
    "class_type": "BizyAirSiliconCloudVLMAPI",
    "_meta": {
      "title": "\u2601\ufe0fBizyAir SiliconCloud VLM API"
    }
  },
  "20": {
    "inputs": {
      "String": "HD photography, HD product rendering, 4K, high quality\uff0c no background,advanced upper design, high-quality product design, exquisite shoelaces, (white background: 1.15)",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32"
    }
  },
  "21": {
    "inputs": {
      "text": [
        "9",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "0",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP\u6587\u672c\u7f16\u7801\u5668"
    }
  },
  "22": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "2",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter\u52a0\u8f7d\u5668"
    }
  },
  "23": {
    "inputs": {
      "aspect_ratio": "original",
      "proportional_width": 1,
      "proportional_height": 1,
      "fit": "letterbox",
      "method": "lanczos",
      "round_to_multiple": "8",
      "scale_to_side": "width",
      "scale_to_length": 1024,
      "background_color": "#ffffff",
      "image": [
        "13",
        0
      ]
    },
    "class_type": "LayerUtility: ImageScaleByAspectRatio V2",
    "_meta": {
      "title": "\u6309\u5bbd\u9ad8\u6bd4\u7f29\u653e_V2"
    }
  },
  "24": {
    "inputs": {
      "lora_stack": [
        "10",
        0
      ],
      "model": [
        "7",
        0
      ],
      "optional_clip": [
        "5",
        0
      ]
    },
    "class_type": "easy loraStackApply",
    "_meta": {
      "title": "\u5e94\u7528 LoRA \u5806"
    }
  },
  "25": {
    "inputs": {
      "String": [
        "15",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32"
    }
  },
  "26": {
    "inputs": {
      "boolean": [
        "16",
        0
      ],
      "on_true": [
        "17",
        0
      ],
      "on_false": [
        "18",
        0
      ]
    },
    "class_type": "Switch any [Crystools]",
    "_meta": {
      "title": "\u5207\u6362\u4efb\u610f"
    }
  },
  "27": {
    "inputs": {
      "prompt": [
        "25",
        0
      ],
      "model": "qwen-plus",
      "system_message": "You are creating a prompt for Stable Diffusion to generate an image. First step: understand the input and generate a text prompt for the input. Second step: only respond in English with the prompt itself in phrase, but embellish it as needed but keep it under 50 tokens.",
      "temperature": 0.7000000000000001,
      "max_tokens": 1024,
      "detail": "auto",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "LLM\u9ad8\u7ea7\u8282\u70b9"
    }
  },
  "28": {
    "inputs": {
      "max_width": 1280,
      "max_height": 1280,
      "min_width": 0,
      "min_height": 0,
      "crop_if_required": "no",
      "images": [
        "23",
        0
      ]
    },
    "class_type": "ConstrainImage|pysssss",
    "_meta": {
      "title": "\u9650\u5236\u56fe\u50cf\u533a\u57df"
    }
  },
  "29": {
    "inputs": {
      "weight": [
        "26",
        0
      ],
      "weight_type": "strong style transfer",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 0.5000000000000001,
      "embeds_scaling": "V only",
      "model": [
        "22",
        0
      ],
      "ipadapter": [
        "22",
        1
      ],
      "image": [
        "14",
        0
      ],
      "image_negative": [
        "23",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "\u5e94\u7528IPAdapter\uff08\u9ad8\u7ea7\uff09"
    }
  },
  "30": {
    "inputs": {
      "preprocessor": "TEEDPreprocessor",
      "resolution": 512,
      "image": [
        "28",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "Aux\u96c6\u6210\u9884\u5904\u7406\u5668"
    }
  },
  "31": {
    "inputs": {
      "text_1": [
        "20",
        0
      ],
      "text_2": [
        "27",
        0
      ]
    },
    "class_type": "FusionText",
    "_meta": {
      "title": "\u878d\u5408\u6587\u672c"
    }
  },
  "32": {
    "inputs": {
      "image": [
        "28",
        0
      ]
    },
    "class_type": "GetImageSize+",
    "_meta": {
      "title": "\u83b7\u53d6\u56fe\u50cf\u5c3a\u5bf8"
    }
  },
  "33": {
    "inputs": {
      "boolean": [
        "16",
        0
      ],
      "on_true": [
        "19",
        0
      ],
      "on_false": [
        "31",
        0
      ]
    },
    "class_type": "Switch any [Crystools]",
    "_meta": {
      "title": "\u5207\u6362\u4efb\u610f"
    }
  },
  "34": {
    "inputs": {
      "width": [
        "32",
        0
      ],
      "height": [
        "32",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "\u7a7aLatent"
    }
  },
  "35": {
    "inputs": {
      "action": "append",
      "tidy_tags": "yes",
      "text_a": "",
      "text_b": "(Pure background:1.5)",
      "text_c": [
        "33",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32\u64cd\u4f5c"
    }
  },
  "36": {
    "inputs": {
      "text": [
        "35",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "0",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP\u6587\u672c\u7f16\u7801\u5668"
    }
  },
  "37": {
    "inputs": {
      "strength": 0.9000000000000001,
      "start_percent": 0,
      "end_percent": 0.9000000000000002,
      "positive": [
        "36",
        0
      ],
      "negative": [
        "21",
        0
      ],
      "control_net": [
        "1",
        0
      ],
      "image": [
        "30",
        0
      ],
      "vae": [
        "0",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNet\u5e94\u7528\uff08\u65e7\u7248\u9ad8\u7ea7\uff09"
    }
  },
  "38": {
    "inputs": {
      "text": [
        "35",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "5",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP\u6587\u672c\u7f16\u7801\u5668"
    }
  },
  "39": {
    "inputs": {
      "seed": [
        "12",
        0
      ],
      "steps": 20,
      "cfg": 2.5,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "29",
        0
      ],
      "positive": [
        "37",
        0
      ],
      "negative": [
        "37",
        1
      ],
      "latent_image": [
        "34",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "K\u91c7\u6837\u5668"
    }
  },
  "40": {
    "inputs": {
      "samples": [
        "39",
        0
      ],
      "vae": [
        "0",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE\u89e3\u7801"
    }
  },
  "41": {
    "inputs": {
      "downsampling_factor": 1,
      "downsampling_function": "bicubic",
      "mode": "keep aspect ratio",
      "weight": 1,
      "autocrop_margin": 0.10000000000000002,
      "conditioning": [
        "38",
        0
      ],
      "style_model": [
        "3",
        0
      ],
      "clip_vision": [
        "4",
        0
      ],
      "image": [
        "40",
        0
      ]
    },
    "class_type": "ReduxAdvanced",
    "_meta": {
      "title": "ReduxAdvanced"
    }
  },
  "42": {
    "inputs": {
      "conditioning": [
        "41",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "\u6761\u4ef6\u96f6\u5316"
    }
  },
  "43": {
    "inputs": {
      "guidance": 3.5,
      "conditioning": [
        "41",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "Flux\u5f15\u5bfc"
    }
  },
  "44": {
    "inputs": {
      "upscale_method": "lanczos",
      "scale_by": 0.7500000000000001,
      "image": [
        "40",
        0
      ]
    },
    "class_type": "ImageScaleBy",
    "_meta": {
      "title": "\u56fe\u50cf\u6309\u7cfb\u6570\u7f29\u653e"
    }
  },
  "45": {
    "inputs": {
      "upscale_model": [
        "6",
        0
      ],
      "image": [
        "44",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "\u56fe\u50cf\u901a\u8fc7\u6a21\u578b\u653e\u5927"
    }
  },
  "46": {
    "inputs": {
      "preprocessor": "PyraCannyPreprocessor",
      "resolution": 512,
      "image": [
        "45",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "Aux\u96c6\u6210\u9884\u5904\u7406\u5668"
    }
  },
  "47": {
    "inputs": {
      "pixels": [
        "45",
        0
      ],
      "vae": [
        "8",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE\u7f16\u7801"
    }
  },
  "48": {
    "inputs": {
      "strength": 0.45000000000000007,
      "start_percent": 0,
      "end_percent": 0.4500000000000001,
      "positive": [
        "43",
        0
      ],
      "negative": [
        "42",
        0
      ],
      "control_net": [
        "11",
        0
      ],
      "image": [
        "46",
        0
      ],
      "vae": [
        "8",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNet\u5e94\u7528\uff08\u65e7\u7248\u9ad8\u7ea7\uff09"
    }
  },
  "49": {
    "inputs": {
      "seed": 1040180462665087,
      "steps": 25,
      "cfg": 1,
      "sampler_name": "uni_pc_bh2",
      "scheduler": "sgm_uniform",
      "denoise": 0.7000000000000002,
      "model": [
        "24",
        0
      ],
      "positive": [
        "48",
        0
      ],
      "negative": [
        "48",
        1
      ],
      "latent_image": [
        "47",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "K\u91c7\u6837\u5668"
    }
  },
  "50": {
    "inputs": {
      "samples": [
        "49",
        0
      ],
      "vae": [
        "8",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE\u89e3\u7801"
    }
  },
  "51": {
    "inputs": {
      "images": [
        "50",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "\u9884\u89c8\u56fe\u50cf"
    }
  },
  "52": {
    "inputs": {
      "width": [
        "13",
        1
      ],
      "height": [
        "13",
        2
      ],
      "crop": "disabled",
      "upscale_method": "nearest-exact",
      "lock_aspect_ratio": true,
      "image": [
        "50",
        0
      ]
    },
    "class_type": "EG_TX_SFBLS",
    "_meta": {
      "title": "2\ud83d\udc15Image scaling lock"
    }
  },
  "53": {
    "inputs": {
      "images": [
        "52",
        0
      ]
    },
    "class_type": "ETN_KritaOutput",
    "_meta": {
      "title": "Krita Output"
    }
  }
}