{
  "0": {
    "inputs": {
      "ckpt_name": "SDXL/\u4ea7\u54c1\u5199\u5b9e_SHMILY\u53bb\u4f2a\u5b58\u771f_V1.0.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Checkpoint\u52a0\u8f7d\u5668(\u7b80\u6613)"
    }
  },
  "1": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "0",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter\u52a0\u8f7d\u5668"
    }
  },
  "2": {
    "inputs": {
      "control_net_name": "sdxl_cn/controlnet-union-sdxl-1.0.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNet\u52a0\u8f7d\u5668"
    }
  },
  "3": {
    "inputs": {
      "style_model_name": "flux1-redux-dev.safetensors"
    },
    "class_type": "StyleModelLoader",
    "_meta": {
      "title": "\u98ce\u683c\u6a21\u578b\u52a0\u8f7d\u5668"
    }
  },
  "4": {
    "inputs": {
      "clip_name": "sigclip_vision_patch14_384.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "CLIP\u89c6\u89c9\u52a0\u8f7d\u5668"
    }
  },
  "5": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp8_e4m3fn.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "\u53ccCLIP\u52a0\u8f7d\u5668"
    }
  },
  "6": {
    "inputs": {
      "model_name": "OmniSR_X2_DIV2K.safetensors"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "\u653e\u5927\u6a21\u578b\u52a0\u8f7d\u5668"
    }
  },
  "7": {
    "inputs": {
      "unet_name": "SuperMerged2.6.safetensors",
      "weight_dtype": "fp8_e4m3fn"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "UNET\u52a0\u8f7d\u5668"
    }
  },
  "8": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "VAE\u52a0\u8f7d\u5668"
    }
  },
  "9": {
    "inputs": {
      "String": "reflect light\uff0ctext, watermark,ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2),Avoid images that are blurry,poorly lit,or lack details. No other objects or distractions in the frame. Avoid low-quality textures,dull colors,and flat lighting,, (EasyNegative:1.2),badhandv4,NSFW, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, (ugly:1.331), (duplicate:1.331), (morbid:1.21), (mutilated:1.21), (tranny:1.331), mutated hands, (poorly drawn hands:1.5), blurry, (bad anatomy:1.21), (bad proportions:1.331), extra limbs, (disfigured:1.331), (missing arms:1.331), (extra legs:1.331), (fused fingers:1.61051), (too many fingers:1.61051), (unclear eyes:1.331), lowers, bad hands, missing fingers, extra digit,bad hands, missing fingers, (((extra arms and legs))),lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32"
    }
  },
  "10": {
    "inputs": {
      "toggle": false,
      "mode": "simple",
      "num_loras": 2,
      "lora_1_name": "flux/yue\u7535\u5546\u5199\u5b9e\u8d85\u7ec6\u817b\u8d28\u611f.safetensors",
      "lora_1_strength": 0.30000000000000004,
      "lora_1_model_strength": 1.0000000000000002,
      "lora_1_clip_strength": 1.0000000000000002,
      "lora_2_name": "flux/\u7ec7\u7269\u7eb9\u7406flux v1000028.safetensors",
      "lora_2_strength": 0.5000000000000001,
      "lora_2_model_strength": 1.0000000000000002,
      "lora_2_clip_strength": 1.0000000000000002,
      "lora_3_name": "None",
      "lora_3_strength": 1.0000000000000002,
      "lora_3_model_strength": 1.0000000000000002,
      "lora_3_clip_strength": 1.0000000000000002,
      "lora_4_name": "None",
      "lora_4_strength": 1.0000000000000002,
      "lora_4_model_strength": 1.0000000000000002,
      "lora_4_clip_strength": 1.0000000000000002,
      "lora_5_name": "None",
      "lora_5_strength": 1.0000000000000002,
      "lora_5_model_strength": 1.0000000000000002,
      "lora_5_clip_strength": 1.0000000000000002,
      "lora_6_name": "None",
      "lora_6_strength": 1.0000000000000002,
      "lora_6_model_strength": 1.0000000000000002,
      "lora_6_clip_strength": 1.0000000000000002,
      "lora_7_name": "None",
      "lora_7_strength": 1.0000000000000002,
      "lora_7_model_strength": 1.0000000000000002,
      "lora_7_clip_strength": 1.0000000000000002,
      "lora_8_name": "None",
      "lora_8_strength": 1.0000000000000002,
      "lora_8_model_strength": 1.0000000000000002,
      "lora_8_clip_strength": 1.0000000000000002,
      "lora_9_name": "None",
      "lora_9_strength": 1.0000000000000002,
      "lora_9_model_strength": 1.0000000000000002,
      "lora_9_clip_strength": 1.0000000000000002,
      "lora_10_name": "None",
      "lora_10_strength": 1.0000000000000002,
      "lora_10_model_strength": 1.0000000000000002,
      "lora_10_clip_strength": 1.0000000000000002
    },
    "class_type": "easy loraStack",
    "_meta": {
      "title": "\u7b80\u6613Lora\u5806"
    }
  },
  "11": {
    "inputs": {
      "control_net_name": "flux_cn/diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "ControlNet\u52a0\u8f7d\u5668"
    }
  },
  "12": {
    "inputs": {},
    "class_type": "ETN_KritaCanvas",
    "_meta": {
      "title": "Krita Canvas"
    }
  },
  "13": {
    "inputs": {
      "name": "1. \u53c2\u8003(tips: \u9009\u62e9\u8981\u53c2\u8003\u7684\u56fe\u7247\u56fe\u5c42)"
    },
    "class_type": "ETN_KritaImageLayer",
    "_meta": {
      "title": "Krita Image Layer"
    }
  },
  "14": {
    "inputs": {
      "seed": 0
    },
    "class_type": "easy seed",
    "_meta": {
      "title": "\u968f\u673a\u79cd"
    }
  },
  "15": {
    "inputs": {
      "seed": 0
    },
    "class_type": "easy seed",
    "_meta": {
      "title": "\u968f\u673a\u79cd"
    }
  },
  "16": {
    "inputs": {
      "prompt": "1. Extract design elements from images and convert them into shoe designs, and describe them.\n2. The material and texture of shoes can only be woven mesh texture.\n3. No need to explain or describe anything else, no output of your thought process, no output of character description, only describe the appearance of the final designed shoe, and only appear text that matches the appearance of the shoe.\n",
      "model": "qwen-vl-max",
      "system_message": "You are an AI assistant capable of analyzing images. Please carefully observe the image and provide a detailed and accurate description based on the user's question.",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "image": [
        "13",
        0
      ]
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "\ud83e\udd16 LLM\u9ad8\u7ea7\u8282\u70b9"
    }
  },
  "17": {
    "inputs": {
      "name": "3. \u63d0\u793a\u8bcd",
      "type": "text",
      "default": "",
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "18": {
    "inputs": {
      "name": "2. \u662f\u5426\u624b\u8f93\u63d0\u793a\u8bcd(tips:\u6253\u5f00\u4e3a\u624b\u8f93\u63d0\u793a\u8bcd\uff0c\u5173\u95ed\u4e3a\u81ea\u52a8\u53cd\u63a8\u63d0\u793a\u8bcd)(on:3. \u63d0\u793a\u8bcd)(off:3. \u63d0\u793a\u8bcd)",
      "type": "toggle",
      "default": false,
      "min": 0,
      "max": 0
    },
    "class_type": "ETN_Parameter",
    "_meta": {
      "title": "Parameter"
    }
  },
  "19": {
    "inputs": {
      "Number": "0"
    },
    "class_type": "Float",
    "_meta": {
      "title": "\u6d6e\u70b9"
    }
  },
  "20": {
    "inputs": {
      "Number": "0.5"
    },
    "class_type": "Float",
    "_meta": {
      "title": "\u6d6e\u70b9"
    }
  },
  "21": {
    "inputs": {
      "text": [
        "9",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "0",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP\u6587\u672c\u7f16\u7801\u5668"
    }
  },
  "22": {
    "inputs": {
      "aspect_ratio": "original",
      "proportional_width": 1,
      "proportional_height": 1,
      "fit": "letterbox",
      "method": "lanczos",
      "round_to_multiple": "8",
      "scale_to_side": "height",
      "scale_to_length": 1024,
      "background_color": "#ffffff",
      "image": [
        "12",
        0
      ]
    },
    "class_type": "LayerUtility: ImageScaleByAspectRatio V2",
    "_meta": {
      "title": "\u6309\u5bbd\u9ad8\u6bd4\u7f29\u653e_V2"
    }
  },
  "23": {
    "inputs": {
      "lora_stack": [
        "10",
        0
      ],
      "model": [
        "7",
        0
      ],
      "optional_clip": [
        "5",
        0
      ]
    },
    "class_type": "easy loraStackApply",
    "_meta": {
      "title": "\u5e94\u7528 LoRA \u5806"
    }
  },
  "24": {
    "inputs": {
      "String": [
        "17",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "String",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32"
    }
  },
  "25": {
    "inputs": {
      "prompt": [
        "24",
        0
      ],
      "model": "qwen-vl-max",
      "system_message": "\u4f60\u9700\u8981\u5c06\u6211\u8f93\u5165\u7684\u4e2d\u6587\u7ffb\u8bd1\u6210\u82f1\u6587",
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "LLMNodeFull",
    "_meta": {
      "title": "\ud83e\udd16 LLM\u9ad8\u7ea7\u8282\u70b9"
    }
  },
  "26": {
    "inputs": {
      "boolean": [
        "18",
        0
      ],
      "on_true": [
        "19",
        0
      ],
      "on_false": [
        "20",
        0
      ]
    },
    "class_type": "easy ifElse",
    "_meta": {
      "title": "\u662f\u5426\u5224\u65ad"
    }
  },
  "27": {
    "inputs": {
      "max_width": 1280,
      "max_height": 1280,
      "min_width": 0,
      "min_height": 0,
      "crop_if_required": "no",
      "images": [
        "22",
        0
      ]
    },
    "class_type": "ConstrainImage|pysssss",
    "_meta": {
      "title": "\u9650\u5236\u56fe\u50cf\u533a\u57df"
    }
  },
  "28": {
    "inputs": {
      "weight": [
        "19",
        0
      ],
      "weight_type": "strong style transfer",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 0.5000000000000001,
      "embeds_scaling": "V only",
      "model": [
        "1",
        0
      ],
      "ipadapter": [
        "1",
        1
      ],
      "image": [
        "13",
        0
      ],
      "image_negative": [
        "22",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "\u5e94\u7528IPAdapter\uff08\u9ad8\u7ea7\uff09"
    }
  },
  "29": {
    "inputs": {
      "preprocessor": "TEEDPreprocessor",
      "resolution": 512,
      "image": [
        "27",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "Aux\u96c6\u6210\u9884\u5904\u7406\u5668"
    }
  },
  "30": {
    "inputs": {
      "boolean": [
        "18",
        0
      ],
      "on_true": [
        "25",
        0
      ],
      "on_false": [
        "16",
        0
      ]
    },
    "class_type": "easy ifElse",
    "_meta": {
      "title": "\u662f\u5426\u5224\u65ad"
    }
  },
  "31": {
    "inputs": {
      "image": [
        "27",
        0
      ]
    },
    "class_type": "GetImageSize+",
    "_meta": {
      "title": "\u83b7\u53d6\u56fe\u50cf\u5c3a\u5bf8"
    }
  },
  "32": {
    "inputs": {
      "action": "append",
      "tidy_tags": "yes",
      "text_a": "",
      "text_b": "(Pure background:1.5)",
      "text_c": [
        "30",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      }
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "\u5b57\u7b26\u4e32\u64cd\u4f5c"
    }
  },
  "33": {
    "inputs": {
      "text": [
        "32",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "0",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP\u6587\u672c\u7f16\u7801\u5668"
    }
  },
  "34": {
    "inputs": {
      "width": [
        "31",
        0
      ],
      "height": [
        "31",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "\u7a7aLatent"
    }
  },
  "35": {
    "inputs": {
      "strength": 0.9000000000000001,
      "start_percent": 0,
      "end_percent": 0.9500000000000002,
      "positive": [
        "33",
        0
      ],
      "negative": [
        "21",
        0
      ],
      "control_net": [
        "2",
        0
      ],
      "image": [
        "29",
        0
      ],
      "vae": [
        "0",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNet\u5e94\u7528\uff08\u65e7\u7248\u9ad8\u7ea7\uff09"
    }
  },
  "36": {
    "inputs": {
      "text": [
        "32",
        0
      ],
      "speak_and_recognation": {
        "__value__": [
          false,
          true
        ]
      },
      "clip": [
        "5",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP\u6587\u672c\u7f16\u7801\u5668"
    }
  },
  "37": {
    "inputs": {
      "seed": [
        "14",
        0
      ],
      "steps": 20,
      "cfg": 2.5,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "28",
        0
      ],
      "positive": [
        "35",
        0
      ],
      "negative": [
        "35",
        1
      ],
      "latent_image": [
        "34",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "K\u91c7\u6837\u5668"
    }
  },
  "38": {
    "inputs": {
      "samples": [
        "37",
        0
      ],
      "vae": [
        "0",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE\u89e3\u7801"
    }
  },
  "39": {
    "inputs": {
      "downsampling_factor": 1,
      "downsampling_function": "bicubic",
      "mode": "keep aspect ratio",
      "weight": 0.8600000000000002,
      "autocrop_margin": 0.10000000000000002,
      "conditioning": [
        "36",
        0
      ],
      "style_model": [
        "3",
        0
      ],
      "clip_vision": [
        "4",
        0
      ],
      "image": [
        "38",
        0
      ]
    },
    "class_type": "ReduxAdvanced",
    "_meta": {
      "title": "ReduxAdvanced"
    }
  },
  "40": {
    "inputs": {
      "conditioning": [
        "39",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "\u6761\u4ef6\u96f6\u5316"
    }
  },
  "41": {
    "inputs": {
      "guidance": 3.5,
      "conditioning": [
        "39",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "Flux\u5f15\u5bfc"
    }
  },
  "42": {
    "inputs": {
      "upscale_method": "lanczos",
      "scale_by": 0.7500000000000001,
      "image": [
        "38",
        0
      ]
    },
    "class_type": "ImageScaleBy",
    "_meta": {
      "title": "\u56fe\u50cf\u6309\u7cfb\u6570\u7f29\u653e"
    }
  },
  "43": {
    "inputs": {
      "upscale_model": [
        "6",
        0
      ],
      "image": [
        "42",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "\u56fe\u50cf\u901a\u8fc7\u6a21\u578b\u653e\u5927"
    }
  },
  "44": {
    "inputs": {
      "preprocessor": "PyraCannyPreprocessor",
      "resolution": 1024,
      "image": [
        "43",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "Aux\u96c6\u6210\u9884\u5904\u7406\u5668"
    }
  },
  "45": {
    "inputs": {
      "pixels": [
        "43",
        0
      ],
      "vae": [
        "8",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE\u7f16\u7801"
    }
  },
  "46": {
    "inputs": {
      "strength": 0.45000000000000007,
      "start_percent": 0,
      "end_percent": 0.4500000000000001,
      "positive": [
        "41",
        0
      ],
      "negative": [
        "40",
        0
      ],
      "control_net": [
        "11",
        0
      ],
      "image": [
        "44",
        0
      ],
      "vae": [
        "8",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNet\u5e94\u7528\uff08\u65e7\u7248\u9ad8\u7ea7\uff09"
    }
  },
  "47": {
    "inputs": {
      "seed": [
        "15",
        0
      ],
      "steps": 25,
      "cfg": 1,
      "sampler_name": "uni_pc_bh2",
      "scheduler": "sgm_uniform",
      "denoise": 0.6500000000000001,
      "model": [
        "23",
        0
      ],
      "positive": [
        "46",
        0
      ],
      "negative": [
        "46",
        1
      ],
      "latent_image": [
        "45",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "K\u91c7\u6837\u5668"
    }
  },
  "48": {
    "inputs": {
      "samples": [
        "47",
        0
      ],
      "vae": [
        "8",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE\u89e3\u7801"
    }
  },
  "49": {
    "inputs": {
      "width": [
        "12",
        1
      ],
      "height": [
        "12",
        2
      ],
      "crop": "disabled",
      "upscale_method": "nearest-exact",
      "lock_aspect_ratio": true,
      "image": [
        "48",
        0
      ]
    },
    "class_type": "EG_TX_SFBLS",
    "_meta": {
      "title": "2\ud83d\udc15\u56fe\u50cf\u7f29\u653e\u6bd4\u4f8b\u9501"
    }
  },
  "50": {
    "inputs": {
      "images": [
        "48",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "\u9884\u89c8\u56fe\u50cf"
    }
  },
  "51": {
    "inputs": {
      "inputcount": 3,
      "direction": "right",
      "match_image_size": true,
      "Update inputs": null,
      "image_1": [
        "12",
        0
      ],
      "image_2": [
        "13",
        0
      ],
      "image_3": [
        "49",
        0
      ]
    },
    "class_type": "ImageConcatMulti",
    "_meta": {
      "title": "\u56fe\u50cf\u8054\u7ed3\uff08\u591a\u4e2a\uff09"
    }
  },
  "52": {
    "inputs": {
      "width": [
        "12",
        1
      ],
      "height": [
        "12",
        2
      ],
      "crop": "disabled",
      "upscale_method": "nearest-exact",
      "lock_aspect_ratio": true,
      "image": [
        "51",
        0
      ]
    },
    "class_type": "EG_TX_SFBLS",
    "_meta": {
      "title": "2\ud83d\udc15\u56fe\u50cf\u7f29\u653e\u6bd4\u4f8b\u9501"
    }
  },
  "53": {
    "inputs": {
      "images": [
        "49",
        0
      ]
    },
    "class_type": "ETN_KritaOutput",
    "_meta": {
      "title": "Krita Output"
    }
  },
  "54": {
    "inputs": {
      "images": [
        "52",
        0
      ]
    },
    "class_type": "ETN_KritaOutput",
    "_meta": {
      "title": "Krita Output"
    }
  }
}